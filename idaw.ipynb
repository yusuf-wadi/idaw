{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import openai\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\thewa\\AppData\\Local\\Temp\\ipykernel_31928\\1986775532.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
            "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "openai.api_key_path = \"key.txt\"\n",
        "cur_model = \"text-davinci-002\"\n",
        "img = [\"image\", \"picture\", \"photo\", \"landscape\", \"painting\", \"drawing\", \"art\"]\n",
        "\n",
        "\n",
        "# Define Brave path\n",
        "brave_path = \"C:/Program Files/BraveSoftware/Brave-Browser-Nightly/Application/brave.exe\"\n",
        "options = webdriver.ChromeOptions()\n",
        "options.binary_location = brave_path\n",
        "\n",
        "# set dl options\n",
        "prefs = {\"download.default_directory\": \"C:/Users/thewa/Desktop/projects/computational_neuroscience/AI_ML/projects/mun_ching/crawlers/content\"}\n",
        "\n",
        "options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "# Create new automated instance of Brave\n",
        "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
        "driver.set_window_size(100,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "def callGPT(prompt):\n",
        "\n",
        "        completion = openai.Completion.create(\n",
        "                    model=cur_model,\n",
        "                    prompt=prompt,\n",
        "                    temperature=0,\n",
        "                    frequency_penalty=0.5,\n",
        "                    max_tokens=20,\n",
        "\n",
        "\n",
        "                )\n",
        "        \n",
        "        return completion.choices[0].text\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def getGame(response):\n",
        "    \n",
        "    URL = \"https://steamcommunity.com/id/yusufwadi/games?tab=all&xml=1\"\n",
        "    data = requests.get(url=URL)\n",
        "    soup = bs(data.text, 'xml')\n",
        "    names = soup.find_all(\"name\")\n",
        "    \n",
        "    for name in names:\n",
        "        if (SequenceMatcher(a=response,b=name.text).ratio()) > 0.75:\n",
        "            engine.say(\"Now launching... \" + name.text)\n",
        "            engine.runAndWait()\n",
        "            return name.parent.find(\"appID\").text\n",
        "    \n",
        "    print(\"Game not found\")\n",
        "\n",
        "    #print(\"hello\")\n",
        "#getGame(\"noida\")\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "# From here\n",
        "def qCommand(x):\n",
        "\n",
        "    x = x.lower()\n",
        "    r2 = sr.Recognizer()\n",
        "   \n",
        "    if 'youtube' in x:\n",
        "    \n",
        "           \n",
        "            driver.get('https://www.youtube.com/')\n",
        "\n",
        "            with sr.Microphone() as source:\n",
        "                engine.say(\"What do you want to see?\")\n",
        "                engine.runAndWait()\n",
        "                print('')\n",
        "                audio = r2.listen(source)\n",
        "                keyword = r2.recognize_google(audio)\n",
        "                print(keyword)\n",
        "      \n",
        "\n",
        "            YOUR_PROMPT = \"Extract a search query from the following prompt: \" + '\"' + keyword + '\"'\n",
        " \n",
        "            response = callGPT(YOUR_PROMPT)\n",
        "\n",
        "\n",
        "\n",
        "            elem = driver.find_element('xpath', '//input[@id=\"search\"]')\n",
        "            elem.click()\n",
        "            elem.send_keys(response, Keys.RETURN)\n",
        "\n",
        "                \n",
        "\n",
        "            #tryAudio(audio)\n",
        "\n",
        "            # Till here is the code to run a YouTube vid\n",
        "    elif 'google' in x:\n",
        "           \n",
        "            driver.get('https://www.google.com/')\n",
        "\n",
        "            with sr.Microphone() as source:\n",
        "                engine.say(\"What do you want to search for?\")\n",
        "                engine.runAndWait()\n",
        "                print('')\n",
        "                audio = r2.listen(source,timeout=5)\n",
        "                keyword = r2.recognize_google(audio)\n",
        "                print(keyword)\n",
        "            YOUR_PROMPT = \"Extract a search query from the following prompt: \" + '\"' + keyword + '\"'\n",
        "\n",
        "            completion = openai.Completion.create(\n",
        "                model=cur_model,\n",
        "                prompt=YOUR_PROMPT,\n",
        "                temperature=0,\n",
        "                frequency_penalty=0.5,\n",
        "                max_tokens=20,\n",
        "\n",
        "\n",
        "            )\n",
        "\n",
        "            response = callGPT(YOUR_PROMPT)\n",
        "\n",
        "\n",
        "\n",
        "            elem = driver.find_element('xpath', '//input[@class=\"gLFyf gsfi\"]')\n",
        "            elem.click()\n",
        "            elem.send_keys(response, Keys.RETURN)\n",
        "\n",
        "            #tryAudio(audio)\n",
        "            \n",
        "            \n",
        "            # if img in x:\n",
        "\n",
        "                \n",
        "    elif 'play' in x:\n",
        "\n",
        "        print(x)\n",
        "        \n",
        "        prompt = \"Extract the name of the game from the prompt: \" + x\n",
        "\n",
        "        response = callGPT(prompt)\n",
        "\n",
        "        gameID = getGame(response)\n",
        "\n",
        "        driver.get(\"steam://rungameid/\" + gameID)\n",
        "        \n",
        "        driver.switch_to.alert.accept\n",
        "    \n",
        "\n",
        "        #tryAudio(audio)\n",
        "\n",
        "    else:\n",
        "            print(\"command not recognized\")\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listening...\n"
          ]
        },
        {
          "ename": "UnknownValueError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\thewa\\Desktop\\projects\\python\\virutal_assist\\idaw.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m engine\u001b[39m.\u001b[39msay(\u001b[39m\"\u001b[39m\u001b[39mHello. I am Idaw. What can I do for you today?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m engine\u001b[39m.\u001b[39mrunAndWait()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m command \u001b[39m=\u001b[39m defCom(r3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m qCommand(command)\n",
            "\u001b[1;32mc:\\Users\\thewa\\Desktop\\projects\\python\\virutal_assist\\idaw.ipynb Cell 8\u001b[0m in \u001b[0;36mdefCom\u001b[1;34m(r3)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     r3\u001b[39m.\u001b[39madjust_for_ambient_noise(source)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     audio \u001b[39m=\u001b[39m r3\u001b[39m.\u001b[39mlisten(source,phrase_time_limit\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,timeout\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     phrase \u001b[39m=\u001b[39m r3\u001b[39m.\u001b[39;49mrecognize_google(audio)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m command \u001b[39min\u001b[39;00m commands:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m command \u001b[39min\u001b[39;00m phrase:\n",
            "File \u001b[1;32mc:\\Users\\thewa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\speech_recognition\\__init__.py:858\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[1;34m(self, audio_data, key, language, show_all)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39m# return results\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m show_all: \u001b[39mreturn\u001b[39;00m actual_result\n\u001b[1;32m--> 858\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(actual_result, \u001b[39mdict\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(actual_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m, [])) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mraise\u001b[39;00m UnknownValueError()\n\u001b[0;32m    860\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m actual_result[\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    861\u001b[0m     \u001b[39m# return alternative with highest confidence score\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     best_hypothesis \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(actual_result[\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m alternative: alternative[\u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "\u001b[1;31mUnknownValueError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "r3 = sr.Recognizer()\n",
        "\n",
        "engine = pyttsx3.init()\n",
        "voices = engine.getProperty('voices')\n",
        "engine.setProperty('voice', voices[0].id)\n",
        "noCom = True\n",
        "\n",
        "print('Listening...')\n",
        "engine.say(\"Hello. I am Idaw. What can I do for you today?\")\n",
        "engine.runAndWait()\n",
        "\n",
        "with sr.Microphone() as source:\n",
        "        r3.adjust_for_ambient_noise(source,duration=0.5)\n",
        "        audio = r3.listen(source,phrase_time_limit=2)\n",
        "        phrase = r3.recognize_google(audio)\n",
        "\n",
        "qCommand(phrase)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice Assist"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa66cff0e5ae32d77d72462d7ae697218f766852b9c548e64c3db10232013247"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

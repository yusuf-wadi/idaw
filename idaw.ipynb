{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import openai\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\thewa\\AppData\\Local\\Temp\\ipykernel_31928\\1448262854.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
            "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "openai.api_key_path = \"key.txt\"\n",
        "cur_model = \"text-davinci-002\"\n",
        "img = [\"image\", \"picture\", \"photo\", \"landscape\", \"painting\", \"drawing\", \"art\"]\n",
        "\n",
        "\n",
        "# Define Brave path\n",
        "brave_path = \"C:/Program Files/BraveSoftware/Brave-Browser-Nightly/Application/brave.exe\"\n",
        "options = webdriver.ChromeOptions()\n",
        "options.binary_location = brave_path\n",
        "\n",
        "# set dl options\n",
        "prefs = {\"download.default_directory\": \"C:/Users/thewa/Desktop/projects/computational_neuroscience/AI_ML/projects/mun_ching/crawlers/content\"}\n",
        "\n",
        "options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "# Create new automated instance of Brave\n",
        "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
        "driver.minimize_window()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def getGame(response):\n",
        "    \n",
        "    URL = \"https://steamcommunity.com/id/yusufwadi/games?tab=all&xml=1\"\n",
        "    data = requests.get(url=URL)\n",
        "    soup = bs(data.text, 'xml')\n",
        "    names = soup.find_all(\"name\")\n",
        "    \n",
        "    for name in names:\n",
        "        if (SequenceMatcher(a=response,b=name.text).ratio()) > 0.75:\n",
        "            engine.say(\"Now launching... \" + name.text)\n",
        "            engine.runAndWait()\n",
        "            return name.parent.find(\"appID\").text\n",
        "    \n",
        "    print(\"Game not found\")\n",
        "\n",
        "    #print(\"hello\")\n",
        "#getGame(\"noida\")\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tryAudio(audio):\n",
        "    try:\n",
        "        get = r2.recognize_google(audio)\n",
        "        print(get)\n",
        "    except sr.UnknownValueError:\n",
        "        print('Client side error')\n",
        "    except sr.RequestError:\n",
        "        print('Error on googl side')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "# From here\n",
        "def qCommand(x):\n",
        "\n",
        "    x = x.lower()\n",
        "    r2 = sr.Recognizer()\n",
        "   \n",
        "    if 'youtube' in x:\n",
        "    \n",
        "           \n",
        "            driver.get('https://www.youtube.com/')\n",
        "\n",
        "            with sr.Microphone() as source:\n",
        "                engine.say(\"What do you want to see?\")\n",
        "                engine.runAndWait()\n",
        "                print('')\n",
        "                audio = r2.listen(source)\n",
        "                keyword = r2.recognize_google(audio)\n",
        "                print(keyword)\n",
        "      \n",
        "\n",
        "            YOUR_PROMPT = \"Extract a search query from the following prompt: \" + '\"' + keyword + '\"'\n",
        " \n",
        "            response = callGPT(YOUR_PROMPT)\n",
        "\n",
        "\n",
        "\n",
        "            elem = driver.find_element('xpath', '//input[@id=\"search\"]')\n",
        "            elem.click()\n",
        "            elem.send_keys(response, Keys.RETURN)\n",
        "\n",
        "                \n",
        "\n",
        "            tryAudio(audio)\n",
        "\n",
        "            # Till here is the code to run a YouTube vid\n",
        "    elif 'google' in x:\n",
        "           \n",
        "            driver.get('https://www.google.com/')\n",
        "\n",
        "            with sr.Microphone() as source:\n",
        "                engine.say(\"What do you want to search for?\")\n",
        "                engine.runAndWait()\n",
        "                print('')\n",
        "                audio = r2.listen(source,timeout=5)\n",
        "                keyword = r2.recognize_google(audio)\n",
        "                print(keyword)\n",
        "            YOUR_PROMPT = \"Extract a search query from the following prompt: \" + '\"' + keyword + '\"'\n",
        "\n",
        "            completion = openai.Completion.create(\n",
        "                model=cur_model,\n",
        "                prompt=YOUR_PROMPT,\n",
        "                temperature=0,\n",
        "                frequency_penalty=0.5,\n",
        "                max_tokens=20,\n",
        "\n",
        "\n",
        "            )\n",
        "\n",
        "            response = callGPT(YOUR_PROMPT)\n",
        "\n",
        "\n",
        "\n",
        "            elem = driver.find_element('xpath', '//input[@class=\"gLFyf gsfi\"]')\n",
        "            elem.click()\n",
        "            elem.send_keys(response, Keys.RETURN)\n",
        "\n",
        "            tryAudio(audio)\n",
        "            \n",
        "            \n",
        "            # if img in x:\n",
        "\n",
        "                \n",
        "    elif 'play' in x:\n",
        "\n",
        "        print(x)\n",
        "        \n",
        "        prompt = \"Extract the name of the game from the prompt: \" + x\n",
        "\n",
        "        response = callGPT(prompt)\n",
        "\n",
        "        gameID = getGame(response)\n",
        "\n",
        "        driver.get(\"steam://rungameid/\" + gameID)\n",
        "        \n",
        "        elem = driver.switch_to.active_element\n",
        "        elem.send_keys(Keys.RETURN)\n",
        "\n",
        "        tryAudio(audio)\n",
        "\n",
        "    else:\n",
        "            print(\"command not recognized\")\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ignore\n",
        "def defCom(r3):\n",
        "    commands = [\"play\", \"watch\",\"youtube\",\"google\"]\n",
        "\n",
        "    with sr.Microphone() as source:\n",
        "        r3.adjust_for_ambient_noise(source)\n",
        "        audio = r3.listen(source)\n",
        "        phrase = r3.recognize_google(audio)\n",
        "    for command in commands:\n",
        "        if command in phrase:\n",
        "                qCommand(r3.recognize_google(audio))\n",
        "                return False\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listening...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\thewa\\Desktop\\projects\\python\\virutal_assist\\idaw.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39mMicrophone() \u001b[39mas\u001b[39;00m source:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         r3\u001b[39m.\u001b[39madjust_for_ambient_noise(source)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         audio \u001b[39m=\u001b[39m r3\u001b[39m.\u001b[39;49mlisten(source)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         phrase \u001b[39m=\u001b[39m r3\u001b[39m.\u001b[39mrecognize_google(audio)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thewa/Desktop/projects/python/virutal_assist/idaw.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m qCommand(phrase)\n",
            "File \u001b[1;32mc:\\Users\\thewa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\speech_recognition\\__init__.py:652\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m phrase_time_limit \u001b[39mand\u001b[39;00m elapsed_time \u001b[39m-\u001b[39m phrase_start_time \u001b[39m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    650\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(source\u001b[39m.\u001b[39;49mCHUNK)\n\u001b[0;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mbreak\u001b[39;00m  \u001b[39m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    654\u001b[0m frames\u001b[39m.\u001b[39mappend(buffer)\n",
            "File \u001b[1;32mc:\\Users\\thewa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\speech_recognition\\__init__.py:161\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, size):\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyaudio_stream\u001b[39m.\u001b[39;49mread(size, exception_on_overflow\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\thewa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyaudio.py:612\u001b[0m, in \u001b[0;36mStream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[0;32m    609\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    610\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 612\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames, exception_on_overflow)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "r3 = sr.Recognizer()\n",
        "\n",
        "r3.listen_in_background\n",
        "\n",
        "engine = pyttsx3.init()\n",
        "voices = engine.getProperty('voices')\n",
        "engine.setProperty('voice', voices[0].id)\n",
        "noCom = True\n",
        "\n",
        "print('Listening...')\n",
        "engine.say(\"Hello. I am Idaw. What can I do for you today?\")\n",
        "engine.runAndWait()\n",
        "\n",
        "with sr.Microphone() as source:\n",
        "        r3.adjust_for_ambient_noise(source,duration=0.5)\n",
        "        audio = r3.listen(source,phrase_time_limit=2)\n",
        "        phrase = r3.recognize_google(audio)\n",
        "qCommand(phrase)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice Assist"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa66cff0e5ae32d77d72462d7ae697218f766852b9c548e64c3db10232013247"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
